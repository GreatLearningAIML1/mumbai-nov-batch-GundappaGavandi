import warnings 
warnings.filterwarnings('ignore')

# Check the versions of libraries
 
# Python version
import sys
print('Python: {}'.format(sys.version))
# scipy
import scipy
print('scipy: {}'.format(scipy.__version__))
# numpy
import numpy
print('numpy: {}'.format(numpy.__version__))
# matplotlib
import matplotlib
print('matplotlib: {}'.format(matplotlib.__version__))
# pandas
import pandas
print('pandas: {}'.format(pandas.__version__))
# scikit-learn
import sklearn
print('sklearn: {}'.format(sklearn.__version__))

# Load libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from scipy.stats import zscore
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
import seaborn as sns


# Load CSV using Pandas
filename = 'Bank_Personal_Loan_Modelling-1.csv'
#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data_1 = pd.read_csv(filename, sep=',')
print(data_1.shape)

data_1.head(5)

df_1 = data_1.copy()

df_1.info()

df_1.describe(include='all').transpose()

### Check Missing value

df_1.columns

# count the number of NaN values in each column
print(df_1.isnull().sum())

###### Since there is no missing values, thus we go ahead and check the distribution of given data and find out outliars 

# Personal Loan distribution
print(df_1.groupby('Personal Loan').size())

## Target Variable Frequency Distribution
freq = df_1['Personal Loan'].value_counts().to_frame()
freq.reset_index(inplace=True)
freq.columns = [freq.columns[1], 'count']
freq['prop(in %)'] = (freq['count'] / sum(freq['count']))*100
print (freq)
import seaborn as sns
sns.countplot(df_1['Personal Loan'])
plt.show()

  ### Get the target column distribution & comments:
  
  In the above distribution 1 represent number of responder and 0 shows that number of non responder
  
  We are intrested to build a model that will help them identify the potential customers who have higher probability of purchasing the loan. This will increase the success ratio while at the same time reduce the cost of the campaign. Thus we selected out target veriable as 'Personal Loan' in given data set

### Detecting Outliars in Given data set 

# box and whisker plots
df_1.plot(kind='box', subplots=True,layout = (6,4),sharex=False, sharey=False,figsize=(15,20))
plt.show()

plt.plot(df_1["Income"], 'o', color='Blue')
plt.show()

# Income
df_1["Income"].plot.box(grid='True')
plt.show()

# CCAvg
df_1["CCAvg"].plot.box(grid='True')
plt.show()

# Mortgage
df_1["Mortgage"].plot.box(grid='True')
plt.show()

PersonalLoan_stats = df_1.groupby(['Personal Loan']).mean()[['Age','Experience','Income','Family','CCAvg',
                                                           'Education','Mortgage', 'Securities Account',
                                                           'CD Account', 'Online', 'CreditCard']]
PersonalLoan_stats.transpose()

# SubDivided Bar Diagram to se distribution across the data set for Personal Loan responder praportion

PersonalLoan_stats
PersonalLoan_stats.transpose().plot.bar(stacked=True, figsize=(16,8))
plt.show()

#### Comment:
By looking at above graph, We can say that there is huge diffrence between Scaling of the features of data set veriable

Thus Scaling is reqired

We will take a statistical approch to solve this problem

import seaborn as sns
sns.boxplot(x=df_1['Income'])
plt.show()

#### Comment:
 By looking at the above graph We need to give an outlair tratment to this data set

#### scatter plot for two variables from our dataset.

fig, ax = plt.subplots(figsize=(16,8))
ax.scatter(df_1['Income'], df_1['Family'])
ax.set_xlabel('Income')
ax.set_ylabel('Family')
plt.show()

### Create Correlation matrix

plt.figure(figsize=(18,15)) 
sns.heatmap(df_1.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(Parkinson_diseases
plt.show()

#### Comments on Correlation matrix

In above graph/table,

    it is very clear taht,
1. ID will not play an importent roll except uniqe value of a given data records, Thus We Remove from analysis.
2. Age and Experiance are highly corelated with each other thus we use either of them.
3. As Incuome shows some relatopn with other veriable, we keep this in analysis for the timing.
4. ZIP Code is a unique value so either we remove it or convert it in to categorical veriable and use it.
5. Correlation between rest data is some where seems significant with target veriable, but still we have to analyse it and finaly we take some decision to add or remove veriables from the model.

Lets Began with analysis....

#### Prepare a new data set for scaling 

df_2 = data_1.copy()
df_2.columns

df_2.head(5)

X = df_2.drop("ID" , axis=1)
X = X.drop('Experience' , axis=1)
X = X.drop('ZIP Code' , axis=1)

y = df_2.pop("Personal Loan")

X.columns

from scipy import stats
import numpy as np
z = np.abs(stats.zscore(X))
print(z)

threshold = 3
print(np.where(z > 3))

print(z[9][0])

#### Comment :
    Don’t be confused by the results. The first array contains the list of row numbers and second array respective column numbers, which mean z[9][0] have a Z-score higher than 3.
    So, the data point — 09th record on column ZN is an outlier.

#### IQR score -

 Box plot use the IQR method to display data and outliers(shape of the data) but in order to be get a list of identified outlier, we will need to use the mathematical formula and retrieve the outlier data.

 The interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 − Q1.
 In other words, the IQR is the first quartile subtracted from the third quartile; these quartiles can be clearly seen on a box plot on the data.
 It is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers.
 IQR is somewhat similar to Z-score in terms of finding the distribution of data and then keeping some threshold to identify the outlier.
 

#### Comment:

Let’s find out we can box plot uses IQR and how we can use it to find the list of outliers as we did using Z-score calculation. First we will calculate IQR,

Q1 = X.quantile(0.25)
Q3 = X.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

#### Comment:

As we now have the IQR scores, it’s time to get hold on outliers. The below code will give an output with some true and false values. The data point where we have False that means these values are valid whereas True indicates presence of an outlier.

Outliar = (X < (Q1 - 1.5 * IQR)) |(X > (Q3 + 1.5 * IQR))

print('Outliar.shape = ',Outliar.shape)
Outliar.head()

#### Count of Outliars in each column data

Outliar.sum()

#### praportion Of data set is Outliar in percentage

print('Column Nmae' ,'      ' ,'Prportion')
(Outliar.sum()/Outliar.shape[0]) * 100

X["Income"] = X["Income"].replace(X['Income'].loc[Outliar['Income'] == True], X["Income"].median())

X["CCAvg"] = X["CCAvg"].replace(X['CCAvg'].loc[Outliar['CCAvg'] == True], X["CCAvg"].median())

X["Mortgage"] = X["Mortgage"].replace(X['Mortgage'].loc[Outliar['Mortgage'] == True], X["Mortgage"].median())

### Comment on VIF Table:

1. In the above table, ID is irrelevent.
2. Age, Experiance, and ZIP Code, This veriables are higly impacting on Data

Lets try to remove and add the combination of this veriable and see the impact on VIF plot and take the decision for removing veriable from the analysis

df_2 = X.copy()
df_2.columns

# Calculate information value
def calc_iv(df, feature, target, pr=False):
    """
    Set pr=True to enable printing of output.
    
    Output: 
      * iv: float,
      * data: pandas.DataFrame
    """

    lst = []

    df[feature] = df[feature].fillna("NULL")

    for i in range(df[feature].nunique()):
        val = list(df[feature].unique())[i]
        lst.append([feature,                                                        # Variable
                    val,                                                            # Value
                    df[df[feature] == val].count()[feature],                        # All
                    df[(df[feature] == val) & (df[target] == 1)].count()[feature],  # Good (think: Responder == 1)
                    df[(df[feature] == val) & (df[target] == 0)].count()[feature]]) # Bad (think: NonResponder == 0)

    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Good', 'Bad'])

    data['Share'] = data['All'] / data['All'].sum()
    data['Bad Rate'] = data['Bad'] / data['All']
    data['Distribution Good'] = (data['All'] - data['Bad']) / (data['All'].sum() - data['Bad'].sum())
    data['Distribution Bad'] = data['Bad'] / data['Bad'].sum()
    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])

    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})

    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])

    data = data.sort_values(by=['Variable', 'Value'], ascending=[True, True])
    data.index = range(len(data.index))

    if pr:
        print(data)
        print('IV = ', data['IV'].sum())


    iv = data['IV'].sum()
    # print(iv)

    return iv, data

iv, data = calc_iv(df_1,"Income", 'Personal Loan')

iv*100

### Find a below link to understand WOE and IV 

https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html

A = X.copy()
X = X.drop('Personal Loan' , axis=1)

for i in range(X.shape[1]):
    iv, data = calc_iv(A,X.columns[i], 'Personal Loan')
    print(X.columns[i], '=', iv)

X.columns

df_2.rename(columns={'Personal Loan':'Personal_Loan','ZIP Code':'ZIP_Code','CD Account': 'CD_Account','Securities Account':'Securities_Account'},inplace=True)
df_2.head()

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
train_set, test_set, train_labels, test_labels = train_test_split(X, y, test_size=.30, random_state=1)

#X_train = Train Set, X_test = train_labels, y_train = test_set, y_test = test_labels

print("Train Set =", train_set.shape)
print("Test Set =",test_set.shape)
print('test_labels = ',test_labels.shape)
print('train_labels = ', train_labels.shape)

#Model development
#spliting into development, validation and hold-out sample
mydata = df_2.copy()

mydata_dev, mydata_val, mydata_holdout = np.split(
        mydata.sample(frac=1, random_state=1212), 
        [int(.5*len(mydata)), 
         int(.8*len(mydata))]
        )

(len(mydata_dev), len(mydata_val), len(mydata_holdout))

print(round(sum(mydata_dev['Personal_Loan'])*100/len(mydata_dev),1),
    round(sum(mydata_val['Personal_Loan'])*100/len(mydata_val),1),
    round(sum(mydata_holdout['Personal_Loan'])*100/len(mydata_holdout),1))

import statsmodels.formula.api as sm
import statsmodels.api

## Running regression with all the variables
mylogit = sm.glm(formula = """Personal_Loan ~ Income + Family + CCAvg + Education 
        + Mortgage + Securities_Account + CD_Account + Online""" , 
        data = mydata_dev, family=statsmodels.api.families.Binomial()).fit()
mylogit.summary()

import statsmodels.formula.api as sm
import statsmodels.api

## Running regression with all the variables
mylogit = sm.glm(formula = """Personal_Loan ~ Income + Family + CCAvg + Education 
         + CD_Account + Online """ , 
        data = mydata_dev, family=statsmodels.api.families.Binomial()).fit()
mylogit.summary()

#VIF Factor
def VIF(formula,data):
    from patsy import dmatrices
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    y , X = dmatrices(formula,data = data,return_type="dataframe")
    vif = pd.DataFrame()
    vif["Variable"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) 
        for i in range(X.shape[1])]
    return(vif.round(1))

vif=VIF("""Personal_Loan ~ Income + Family + CCAvg + Education 
         + CD_Account + Online """,mydata_dev)
vif 

#VIF is less than 2, hence the model is robust now

## Predicting Probabilities
mydata_dev["prob"] = mylogit.predict(mydata_dev)## Classification
mydata_dev.prob.quantile(
  [0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99,1])

mydata_dev["class"] = mydata_dev["prob"].map(
        lambda x: 1 if x > mydata_dev.prob.quantile(0.95) else 0)

mydata_dev["class"].head()

mydata_dev['decile']=pd.qcut(mydata_dev.prob, 10, labels=False)
## Rank Order Table and KS Statistics
def Response_Rate(X,y,Target):
    
    Rank=X.groupby('decile').apply(lambda x: pd.Series([
        np.min(x[y]),
        np.max(x[y]),
        np.mean(x[y]),
        np.size(x[y]),
        np.sum(x[Target]),
        np.size(x[Target][x[Target]==0]),
        ],
        index=(["min_prob","max_prob","avg_prob",
                "cnt","cnt_resp","cnt_non_resp"])
        )).reset_index()
    Rank=Rank.sort_values(by='decile',ascending=False)
    Rank["rrate"]=round(Rank["cnt_resp"]*100/Rank["cnt"],2)
    Rank["cum_resp"]=np.cumsum(Rank["cnt_resp"])
    Rank["cum_non_resp"]=np.cumsum(Rank["cnt_non_resp"])
    Rank["cum_resp_pct"]=round(Rank["cum_resp"]*100/np.sum(Rank["cnt_resp"]),2)
    Rank["cum_non_resp_pct"]=round(
            Rank["cum_non_resp"]*100/np.sum(Rank["cnt_non_resp"]),2)
    Rank["KS"] = round(Rank["cum_resp_pct"] - Rank["cum_non_resp_pct"],2)
    Rank
    return(Rank)

RRate = Response_Rate(mydata_dev,"prob","Personal_Loan")
RRate

mydata_dev.head()

#Roc_Curve and KS
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(mydata_dev["Personal_Loan"],mydata_dev["prob"] )
# Plot ROC curve
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()

KS = (tpr - fpr).max()
KS

from sklearn.metrics import roc_auc_score
auc = roc_auc_score(mydata_dev["Personal_Loan"],mydata_dev["prob"])
auc

#Confusion Matrix and Classification Accuracy
from sklearn.metrics import confusion_matrix, accuracy_score   
confusion_matrix = (
    {"conf_matx": confusion_matrix(mydata_dev['Personal_Loan'],mydata_dev["class"]),
     "accuracy": accuracy_score(mydata_dev['Personal_Loan'],mydata_dev["class"])
    })

print("confusion matrix \n" , confusion_matrix["conf_matx"], 
      "\n\nclassification accuracy ", confusion_matrix["accuracy"]
      )

from sklearn.neighbors import KNeighborsClassifier
from scipy.stats import zscore
from sklearn.preprocessing import Imputer
from sklearn.metrics import accuracy_score

kbankdf = df_2

## Target Variable Frequency Distribution
freq = kbankdf['Personal_Loan'].value_counts().to_frame()
freq.reset_index(inplace=True)
freq.columns = [freq.columns[1], 'count']
freq['prop'] = freq['count'] / sum(freq['count'])
freq

NNH = KNeighborsClassifier(n_neighbors = 21, weights = 'uniform', 
                           metric = 'euclidean')

## scaling all variables
bankdf_z = kbankdf.apply(zscore)
bankdf_z.shape

## Fit the model
NNH.fit(train_set, train_labels)

## Get the probability
tempkdf = pd.DataFrame(NNH.predict_proba(train_set))[1]

knndf = pd.DataFrame(tempkdf)
knndf['prob'] = pd.DataFrame(tempkdf)

knndf['prob'] = np.where(knndf['prob'] >= 0.096, 1, 0)

#Confusion Matrix and Classification Accuracy
from sklearn.metrics import confusion_matrix, accuracy_score   
confusion_matrix = (
    {"conf_matx": confusion_matrix(train_labels,knndf["prob"]),
     "accuracy": accuracy_score(train_labels,knndf["prob"])
    })

print("confusion matrix \n" , confusion_matrix["conf_matx"], 
      "\n\nclassification accuracy ", confusion_matrix["accuracy"]
      )

nbbankdf = df_2

from sklearn.naive_bayes import BernoulliNB
NB = BernoulliNB()
NB.fit(train_set, train_labels)
#train_set, test_set, train_labels, test_labels

tempdf = pd.DataFrame(NB.predict_proba(train_set))[1]

nbbankdf = pd.DataFrame(tempdf)

nbbankdf['prob'] = pd.DataFrame(tempdf)
nbbankdf['prob'].shape

nbbankdf['prob'] = np.where(nbbankdf['prob'] >= 0.096, 1, 0)
pd.DataFrame(nbbankdf['prob']).shape

#Confusion Matrix and Classification Accuracy
from sklearn.metrics import confusion_matrix, accuracy_score   
confusion_matrix = (
    {"conf_matx": confusion_matrix(train_labels,nbbankdf['prob']),
     "accuracy": accuracy_score(train_labels,nbbankdf['prob'])
    })

print("confusion matrix \n" , confusion_matrix["conf_matx"], 
      "\n\nclassification accuracy ", confusion_matrix["accuracy"]
      )

#### Apply the Decision tree model and print the accuracy of DecisionTree Model

# calculate accuracy measures and confusion matrix
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_extraction.text import CountVectorizer #DT does not take strings as input for the model fit step....
dt_model = DecisionTreeClassifier(criterion = 'entropy' )

dt_model.fit(train_set, train_labels)

# importance of features in the tree building ( The importance of a feature is computed as the 
#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )
print (pd.DataFrame(dt_model.feature_importances_, columns = ["Imp"], index = train_set.columns))

y_predict = dt_model.predict(test_set)

#### Print the feature importance of the decision model - Optional

print(dt_model.score(train_set , train_labels))
print(dt_model.score(test_set , test_labels))

print(metrics.confusion_matrix(test_labels, y_predict))

#### Apply the Random forest model and print the accuracy of Random forest Model

from sklearn.ensemble import RandomForestClassifier
rfcl = RandomForestClassifier(n_estimators = 50)
rfcl = rfcl.fit(train_set, train_labels)

y_predict = rfcl.predict(test_set)
print(rfcl.score(test_set , test_labels))
print(metrics.confusion_matrix(test_labels, y_predict))

metrics.precision_score(test_labels,y_predict)

metrics.recall_score(test_labels,y_predict)

####  Apply Adaboost Ensemble Algorithm for the same data and print the accuracy.

from sklearn.ensemble import AdaBoostClassifier
abcl = AdaBoostClassifier(base_estimator=dt_model, n_estimators=50)
#abcl = AdaBoostClassifier( n_estimators=50)
abcl = abcl.fit(train_set, train_labels)

y_predict = abcl.predict(test_set)
print(abcl.score(test_set , test_labels))

print(metrics.confusion_matrix(test_labels, y_predict))

metrics.precision_score(test_labels,y_predict)

metrics.recall_score(test_labels,y_predict)

#### Apply Bagging Classifier Algorithm and print the accuracy. 

from sklearn.ensemble import BaggingClassifier

bgcl = BaggingClassifier(base_estimator=dt_model, n_estimators=50)

#bgcl = BaggingClassifier(n_estimators=50)
bgcl = bgcl.fit(train_set, train_labels)

y_predict = bgcl.predict(test_set)

print(bgcl.score(test_set , test_labels))

print(metrics.confusion_matrix(test_labels, y_predict))

metrics.precision_score(test_labels,y_predict)

metrics.recall_score(test_labels,y_predict)

#### Apply GradientBoost Classifier Algorithm for the same data and print the accuracy 

from sklearn.ensemble import GradientBoostingClassifier
gbcl = GradientBoostingClassifier(n_estimators = 50)
gbcl = gbcl.fit(train_set, train_labels)

y_predict = gbcl.predict(test_set)
print(gbcl.score(test_set , test_labels))
print(metrics.confusion_matrix(test_labels, y_predict))

metrics.precision_score(test_labels,y_predict)

metrics.recall_score(test_labels,y_predict)

#### Apply Voting Classfier on the given dataset and state your insights.

from sklearn.ensemble import VotingClassifier
from sklearn.metrics import log_loss
from collections import Counter
eclf = VotingClassifier(estimators=[
    ('rf1', rfcl),('gbc', gbcl), ('adb',abcl)], voting='soft', weights = [1,3,1])

eclf.fit(train_set, train_labels)
y_val_pred = eclf.predict_proba(test_set)
print("log_loss: ",log_loss(test_labels, y_val_pred))
confidence = eclf.score(test_set, test_labels)
print('accuracy:',confidence)
predictions = eclf.predict(test_set)
print('predicted class counts:',Counter(predictions))

#Greadent Booster classigier is good model
