{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated_R7_ExternalLab_Questions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4WH1Pr4KQlCh"
      },
      "source": [
        "### Build a DNN using Keras with `RELU` and `ADAM`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TbvI8LqlQlCl",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SPW-a-qYQlCp",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import cifar10, mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = (15, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "74cQBsi5QlCw",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#### Collect Fashion mnist data from tf.keras.datasets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wVWy0oDTr2Kj",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "# Load/Prep the Data\n",
        "(x_train, y_train_num), (x_test, y_test_num) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "no7aWYZyQlC1",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#### Change train and test labels into one-hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UX6otc4wQlC2",
        "colab": {}
      },
      "source": [
        "y_train = np_utils.to_categorical(y_train_num, 10)\n",
        "y_test = np_utils.to_categorical(y_test_num, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QjNrRTdoQlC5",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#### Build the Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CDJ9DHVNQlC7"
      },
      "source": [
        "#### Initialize model, reshape & normalize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCDQs_g1QlC8",
        "outputId": "3d5e6329-7087-4067-90de-d530b7c7d7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('--- THE DATA ---')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- THE DATA ---\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kBGwTTilQlDD",
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "#### Add two fully connected layers with 200 and 100 neurons respectively with `relu` activations. Add a dropout layer with `p=0.25`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IXbfpfOzQlDF",
        "colab": {}
      },
      "source": [
        "TRAIN = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRthSoe05_2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Reshape((784,), input_shape=(28, 28, 1)))\n",
        "\n",
        "model.add(Dense(200, activation='relu', input_shape=(784,)))   #First hidden layer of 512 neurons, each neuron takes input \n",
        "                                                               # vector of size 784\n",
        "\n",
        "\n",
        "model.add(Dense(100, activation='relu'))\n",
        "# Dropout\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5I8f5otcQlDJ"
      },
      "source": [
        "### Add the output layer with a fully connected layer with 10 neurons with `softmax` activation. Use `categorical_crossentropy` loss and `adam` optimizer and train the network. And, report the final validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEEk1Tr0-cn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))            # Adding a softmax layer for output which contains as many \n",
        "                                                               # neurons as the number of classes (10) which is also the \n",
        "                                                               # the shape of each output vector ( one hot coded)\n",
        "\n",
        "                                                               # output layer also uses softmax. This normalizes the values \n",
        "                                                               # from the ten output nodes such that: \n",
        "                                                               #        all the values are between 0 and 1, and\n",
        "                                                               #        the sum of all ten values is 1.  \n",
        "                                                               # prediction is the lable of the node that gets highest fraction, is "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2hFfUuy7UY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128    # keep in 2^x \n",
        "epochs = 20\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC7QBMKS7yD3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "8fab59af-fa35-490a-aa7e-f6995fa9b2b6"
      },
      "source": [
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=EPOCHS,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.2532 - acc: 0.9244 - val_loss: 0.1120 - val_acc: 0.9629\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.1080 - acc: 0.9670 - val_loss: 0.1037 - val_acc: 0.9693\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0770 - acc: 0.9769 - val_loss: 0.0714 - val_acc: 0.9785\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 9s 153us/step - loss: 0.0598 - acc: 0.9810 - val_loss: 0.0768 - val_acc: 0.9758\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0478 - acc: 0.9846 - val_loss: 0.0682 - val_acc: 0.9802\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0404 - acc: 0.9869 - val_loss: 0.0796 - val_acc: 0.9777\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0358 - acc: 0.9885 - val_loss: 0.0917 - val_acc: 0.9777\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0297 - acc: 0.9901 - val_loss: 0.0814 - val_acc: 0.9806\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0266 - acc: 0.9909 - val_loss: 0.0776 - val_acc: 0.9826\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 163us/step - loss: 0.0244 - acc: 0.9924 - val_loss: 0.0835 - val_acc: 0.9816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f50aa353978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRX_rQFW7Vz3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39471b90-db81-4f46-d7c6-4cd84af9d081"
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=False)\n",
        "print(score)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.08347939973971125, 0.9816]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}